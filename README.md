Multi-threaded JSON Processing Pipeline in C++
A simple, modular, and extensible multi-threaded pipeline system designed to process streaming data (e.g., JSON logs) from multiple sources concurrently using lock-free queues. This project demonstrates a robust processing architecture with clear extensibility.

Features
Modular architecture: decoupled stages and sources
Multi-threaded pipeline stages
Lock-free communication using boost::lockfree::queue
Structured logging with spdlog
Simulated data generation with live file tailing
Extensible via CLI, JSON config, or new stages

🏗️ Architecture Overview
1. Process Model
Each pipeline stage runs in its own thread

Uses boost::lockfree::queue for thread-safe message passing

Future-ready for multi-process isolation (via IPC or sockets)

2. Communication Model
Data is wrapped in a DataPacket struct

Communication uses in-memory lock-free queues

3. Configuration & Orchestration
Hardcoded for now, but supports JSON-based configuration

Orchestration controlled by a central main.cpp that initializes:

Multiple file sources

Pipeline stages

Logger

🧩 Core Components

InputSource
Abstract base class: InputSource

Derived implementations:
FileSource (used)
SocketSource (planned)
SensorSource (planned)

PipelineStage
Abstract base class: PipelineStage

Each stage overrides Process(std::shared_ptr<DataPacket>)
Stages used:

JsonParserStage
FilterStage
EnricherStage
TransformStage
FlinkStage

🔧 Dependencies
Boost: lock-free queues, filesystem

spdlog: fast and thread-safe logging

🧪 How to Build and Run
🧱 Directory Structure
.
├── include/         # All .hpp header files
├── src/             # All .cpp implementation files
├── build/           # Output build directory (created manually)
├── test/            # Testing assets & output
│   ├── pipeline         # Compiled binary
│   ├── input.txt        # Sample input file 1
│   ├── input1.txt       # Sample input file 2
│   ├── input2.txt       # Sample input file 3
│   ├── generate.sh      # Script to simulate data stream
│   └── pipeline.log     # Logs generated by spdlog
├── CMakeLists.txt
└── README.md
🛠 Build Instructions

bash
# Clone the repo
git clone https://github.com/SyedAmeen276/Pipeline.git
cd Pipeline

# Create build directory and compile
mkdir -p build
cd build
cmake ..
make
This will produce a binary named pipeline.

▶️ Run the Pipeline
bash
Copy
Edit
./pipeline
By default, it reads from 3 hardcoded files:

input.txt

input1.txt

input2.txt

🧪 Simulate Live Input
In a separate terminal, run:

bash
./generate.sh
This will randomly append JSON lines to the input files.

📄 Output
Processed results and stage logs are stored in:

pipeline.log
You’ll also see console logs via spdlog.

🧭 Future Extensions
🩺 Health checks via HTTP or CLI

🧵 Dynamic thread pool per stage

📈 Metrics + Prometheus exporter

📦 Plugin-based stage loading

🛠 Live configuration reloading

📌 Summary
This project provides a clear and extendable foundation for building high-performance data pipelines in C++. It uses a multi-threaded model with decoupled stages, is easy to extend, and introduces important architectural patterns for real-time streaming systems.
