Multi-threaded JSON Processing Pipeline in C++
A simple, modular, and extensible multi-threaded pipeline system designed to process streaming data (e.g., JSON logs) from multiple sources concurrently using lock-free queues. This project demonstrates a robust processing architecture with clear extensibility.

Features
Modular architecture: decoupled stages and sources
Multi-threaded pipeline stages
Lock-free communication using boost::lockfree::queue
Structured logging with spdlog
Simulated data generation with live file tailing
Extensible via CLI, JSON config, or new stages

ğŸ—ï¸ Architecture Overview
1. Process Model
Each pipeline stage runs in its own thread

Uses boost::lockfree::queue for thread-safe message passing

Future-ready for multi-process isolation (via IPC or sockets)

2. Communication Model
Data is wrapped in a DataPacket struct

Communication uses in-memory lock-free queues

3. Configuration & Orchestration
Hardcoded for now, but supports JSON-based configuration

Orchestration controlled by a central main.cpp that initializes:

Multiple file sources

Pipeline stages

Logger

ğŸ§© Core Components

InputSource
Abstract base class: InputSource

Derived implementations:
FileSource (used)
SocketSource (planned)
SensorSource (planned)

PipelineStage
Abstract base class: PipelineStage

Each stage overrides Process(std::shared_ptr<DataPacket>)
Stages used:

JsonParserStage
FilterStage
EnricherStage
TransformStage
FlinkStage

ğŸ”§ Dependencies
Boost: lock-free queues, filesystem

spdlog: fast and thread-safe logging

ğŸ§ª How to Build and Run
ğŸ§± Directory Structure
.
â”œâ”€â”€ include/         # All .hpp header files
â”œâ”€â”€ src/             # All .cpp implementation files
â”œâ”€â”€ build/           # Output build directory (created manually)
â”œâ”€â”€ test/            # Testing assets & output
â”‚   â”œâ”€â”€ pipeline         # Compiled binary
â”‚   â”œâ”€â”€ input.txt        # Sample input file 1
â”‚   â”œâ”€â”€ input1.txt       # Sample input file 2
â”‚   â”œâ”€â”€ input2.txt       # Sample input file 3
â”‚   â”œâ”€â”€ generate.sh      # Script to simulate data stream
â”‚   â””â”€â”€ pipeline.log     # Logs generated by spdlog
â”œâ”€â”€ CMakeLists.txt
â””â”€â”€ README.md
ğŸ›  Build Instructions

bash
# Clone the repo
git clone https://github.com/SyedAmeen276/Pipeline.git
cd Pipeline

# Create build directory and compile
mkdir -p build
cd build
cmake ..
make
This will produce a binary named pipeline.

â–¶ï¸ Run the Pipeline
bash
Copy
Edit
./pipeline
By default, it reads from 3 hardcoded files:

input.txt

input1.txt

input2.txt

ğŸ§ª Simulate Live Input
In a separate terminal, run:

bash
./generate.sh
This will randomly append JSON lines to the input files.

ğŸ“„ Output
Processed results and stage logs are stored in:

pipeline.log
Youâ€™ll also see console logs via spdlog.

ğŸ§­ Future Extensions
ğŸ©º Health checks via HTTP or CLI

ğŸ§µ Dynamic thread pool per stage

ğŸ“ˆ Metrics + Prometheus exporter

ğŸ“¦ Plugin-based stage loading

ğŸ›  Live configuration reloading

ğŸ“Œ Summary
This project provides a clear and extendable foundation for building high-performance data pipelines in C++. It uses a multi-threaded model with decoupled stages, is easy to extend, and introduces important architectural patterns for real-time streaming systems.
